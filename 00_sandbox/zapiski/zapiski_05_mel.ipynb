{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:18:33.792818Z",
     "start_time": "2025-06-16T00:18:33.581762Z"
    }
   },
   "source": [
    "# Base\n",
    "import librosa # alternativa pyAudioAnalysis ali audioFlux\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "# Preprocessing, Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Keras, Classification\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mh5py\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtime\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdatetime\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'h5py'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "fn = f'./genres/rock/rock.00000.wav'\n",
    "sig, sr = librosa.load(fn, mono=True, duration=5)\n",
    "\n",
    "# Try Mel - Parameters!\n",
    "frame_length = int(0.010 * sr)\n",
    "frame_step = int(0.005 * sr)\n",
    "mel_spec = librosa.feature.melspectrogram(y=sig, sr=sr, n_fft=512, hop_length=frame_step, win_length=frame_length, window='hann', n_mels=20, fmin=100, fmax=4000)\n",
    "\n",
    "img = librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), fmin=100, fmax=4000)\n",
    "\n",
    "# Shapes\n",
    "print('Mel-Scpectrogram:', np.shape(mel_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "genres = np.array('pop rock classical blues country disco metal jazz reggae hiphop'.split())\n",
    "n_genres = 2 # Only two genres used #len(genres)\n",
    "n_genres_files = 100 \n",
    "filter_size = 20\n",
    "n_windows = 1003 # For 5 second signal chunk - number of timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NumberOfSignalParts,NumberOfWindows,NumberOfFilters]\n",
    "data = np.zeros((n_genres * n_genres_files, filter_size, n_windows))\n",
    "\n",
    "# [NumberOfSignalParts,1]\n",
    "data_labels = np.zeros((n_genres * n_genres_files, 1))\n",
    "\n",
    "# Dataset - Will take some time to generate\n",
    "data_index = 0\n",
    "for i_genre in range(0, n_genres):\n",
    "    print(f\"genre: {genres[i_genre]}\")\n",
    "    for filename in os.listdir(f'./genres/{genres[i_genre]}'):\n",
    "        fn = f'./genres/{genres[i_genre]}/{filename}'\n",
    "        \n",
    "        # There is one problematic file - format problem (can try ffmpeg decoder)\n",
    "        try:\n",
    "            # Load file (sig-signal; sr-sampling rate)\n",
    "            sig, sr = librosa.load(fn, mono=True, duration=5)\n",
    "\n",
    "            # For demo we will only use first 5 seconds of audio\n",
    "            # Change this!\n",
    "            # Be careful - the size of training data defines later usage\n",
    "\n",
    "            mel_spec = librosa.feature.melspectrogram(y=sig, sr=sr, n_fft=512, hop_length=frame_step, win_length=frame_length, window='hann', n_mels=20, fmin=100, fmax=4000)\n",
    "\n",
    "            # Features - Data\n",
    "            data[data_index] = mel_spec\n",
    "\n",
    "            # Genre - Label\n",
    "            data_labels[data_index] = i_genre\n",
    "\n",
    "            data_index = data_index + 1\n",
    "        except:\n",
    "            print(\"error\")   \n",
    "\n",
    "# Save to h5 file\n",
    "hf = h5py.File('dataset_mel.h5', 'w')\n",
    "hf.create_dataset('data', data=data)\n",
    "hf.create_dataset('data_labels', data=data_labels)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from h5 file\n",
    "hf = h5py.File('dataset_mel.h5', 'r')\n",
    "\n",
    "data = hf.get('data')\n",
    "data = np.array(data)\n",
    "\n",
    "data_labels = hf.get('data_labels')\n",
    "data_labels = np.array(data_labels)\n",
    "\n",
    "print('Data size:', np.shape(data))\n",
    "print('Data_labels size:', np.shape(data_labels))\n",
    "\n",
    "hf.close()\n",
    "\n",
    "img = librosa.display.specshow(librosa.power_to_db(data[0,:], ref=np.max), fmin=100, fmax=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x = np.reshape(data, (data.shape[0]*data.shape[1], data.shape[2]))\n",
    "X = np.reshape(scaler.fit_transform(np.array(x, dtype = float)), data.shape)\n",
    "\n",
    "# Split into test and train\n",
    "# Why stratify=data_labels?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data_labels, test_size=0.2, stratify=data_labels)\n",
    "\n",
    "# Split into train and valid\n",
    "# Why stratify=y_train?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train)\n",
    "\n",
    "# Sizes\n",
    "print('Original dims')\n",
    "print('Train X:', np.shape(X_train))\n",
    "print('Train Y:', np.shape(y_train))\n",
    "print('Test X:', np.shape(X_test))\n",
    "print('Test Y:', np.shape(y_test))\n",
    "print('Val X:', np.shape(X_val))\n",
    "print('Val Y:', np.shape(y_val))\n",
    "\n",
    "# Why correction?\n",
    "print('Corrected dims')\n",
    "X_train = np.expand_dims(X_train, 3)\n",
    "X_test = np.expand_dims(X_test, 3)\n",
    "X_val = np.expand_dims(X_val, 3)\n",
    "\n",
    "print('Train X:', np.shape(X_train))\n",
    "print('Train Y:', np.shape(y_train))\n",
    "print('Test X:', np.shape(X_test))\n",
    "print('Test Y:', np.shape(y_test))\n",
    "print('Val X:', np.shape(X_val))\n",
    "print('Val Y:', np.shape(y_val))\n",
    "\n",
    "plt.hist(y_train, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_test, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_val, bins=n_genres, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the model - add extra layers, change the number of neurons, number of filters, etc...\n",
    "\n",
    "# NN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Input(X_train.shape[1:]))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(rate=0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(n_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # Computes the crossentropy loss between the labels and predictions\n",
    "metr = keras.metrics.SparseCategoricalAccuracy() # Calculates how often predictions match integer labels\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[metr])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping criterion to avoid overfitting\n",
    "# patience: Number of epochs with no improvement after which training will be stopped.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Save best weights\n",
    "model_checkpoint = ModelCheckpoint(\"cnn.weights.h5\", save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Train\n",
    "t_epochs = 10 # Needs to be tuned\n",
    "b_size = 32 # Needs to be tuned as well - What is batch_size?\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=t_epochs, batch_size=b_size, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights(\"cnn.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets observe the loss metric on both the training (blue) and validation (orange) set\n",
    "# What do we noice?\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to evaluate our model on train and test data\n",
    "\n",
    "# Train NN\n",
    "loss, acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Acc train NN: %.3f' % acc)\n",
    "\n",
    "# Test NN\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Acc test NN: %.3f' % acc)\n",
    "\n",
    "# Val NN\n",
    "loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Acc val NN: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NN\n",
    "# Predictions for additional analysis\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "conf = confusion_matrix(y_test, predicted_labels, normalize=\"pred\") # Normalize pred! Explain why?\n",
    "\n",
    "# Visualise confusion matrix\n",
    "plt.figure()\n",
    "plt.imshow(conf)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.yticks(np.arange(n_genres), genres[0:2])\n",
    "plt.xticks(np.arange(n_genres), genres[0:2], rotation='vertical')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:22:08.316732Z",
     "start_time": "2025-06-22T18:22:04.793853Z"
    }
   },
   "source": [
    "# Base\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import h5py\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing, Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# CUDA: https://pytorch.org/get-started/locally/\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchinfo'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[34;01mos\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[34;01mh5py\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[34;01mtorchinfo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m summary\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[34;01msounddevice\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[34;01msd\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torchinfo'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:22:08.374873878Z",
     "start_time": "2025-06-20T01:56:20.204265Z"
    }
   },
   "source": [
    "# Parameters\n",
    "fs = 8000 # Sampling frequency\n",
    "min_rec_len = 8032 # Minimum samples of recording - STFT output dimensions should be divisible by factor 32 (first conv layer no. of filters)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:22:08.378078030Z",
     "start_time": "2025-06-20T01:56:22.152340Z"
    }
   },
   "source": [
    "# Datase loading\n",
    "def read_dataset(dataset):\n",
    "\n",
    "    data = []\n",
    "    data_prop = { \"fs\": [], \"len\": []}\n",
    "\n",
    "    for path, _, files in os.walk(dataset):\n",
    "        for name in files:\n",
    "\n",
    "            # Skip transcriptions\n",
    "            if \"txt\" not in name.lower(): \n",
    "\n",
    "                # Load file (sig-signal; sr-sampling rate) and downsample to 8 kHz\n",
    "                y, sr = librosa.load(os.path.join(path, name), sr=fs)\n",
    "\n",
    "                data_prop[\"fs\"].append(sr)\n",
    "                data_prop[\"len\"].append(len(y) / sr)\n",
    "\n",
    "                # Use whole signal!\n",
    "\n",
    "                # Check length\n",
    "                # Signals can be shorter than min requested\n",
    "                if len(y) < min_rec_len:\n",
    "                    print(f\"Recording {name} is too short ({len(y)}) - padding with zeros ({min_rec_len - len(y)})\")\n",
    "                    y = np.append(y, np.zeros((min_rec_len - len(y), )))\n",
    "\n",
    "                # Creating UrbanSound8K frames of min_rec_len\n",
    "                for i in range (0, y.shape[0] - min_rec_len, min_rec_len):\n",
    "                    data.append(y[i:i + min_rec_len])\n",
    "\n",
    "    print(dataset)\n",
    "    print(np.shape(data))\n",
    "    print(f\"Sampling freq: {np.mean(data_prop['fs'])} [{np.min(data_prop['fs'])} - {np.max(data_prop['fs'])}] Hz\")\n",
    "    print(f\"Length: {np.mean(data_prop['len']) :.2f} [{np.min(data_prop['len']) :.2f} - {np.max(data_prop['len']) :.2f}] s\")\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:22:08.385823166Z",
     "start_time": "2025-06-20T01:57:06.499237Z"
    }
   },
   "source": [
    "# Load all recordings from LibriSpeech-dev\n",
    "# Large-scale (1000 hours) corpus of read English speech\n",
    "# http://openslr.elda.org/resources/12/dev-clean.tar.gz\n",
    "data_libri = read_dataset(\"./librispeech/\")\n",
    "\n",
    "# Save to h5 file\n",
    "hf = h5py.File('dataset_raw.h5', 'w')\n",
    "hf.create_dataset('data_libri', data=data_libri)\n",
    "hf.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./librispeech/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load all recordings from LibriSpeech-dev\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Large-scale (1000 hours) corpus of read English speech\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# http://openslr.elda.org/resources/12/dev-clean.tar.gz\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m data_libri \u001B[38;5;241m=\u001B[39m \u001B[43mread_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./librispeech/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Save to h5 file\u001B[39;00m\n\u001B[1;32m      7\u001B[0m hf \u001B[38;5;241m=\u001B[39m h5py\u001B[38;5;241m.\u001B[39mFile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset_raw.h5\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 32\u001B[0m, in \u001B[0;36mread_dataset\u001B[0;34m(dataset)\u001B[0m\n\u001B[1;32m     29\u001B[0m                 data\u001B[38;5;241m.\u001B[39mappend(y[i:i \u001B[38;5;241m+\u001B[39m min_rec_len])\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset)\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mshape(data))\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSampling freq: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(data_prop[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfs\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmin(data_prop[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfs\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmax(data_prop[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfs\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] Hz\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(data_prop[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlen\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmin(data_prop[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlen\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmax(data_prop[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlen\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] s\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from h5 file\n",
    "hf = h5py.File('dataset_raw.h5', 'r')\n",
    "\n",
    "data_libri = hf.get('data_libri')\n",
    "data_libri = np.array(data_libri)\n",
    "\n",
    "# Samples, Length\n",
    "print('data_libri size:', np.shape(data_libri))\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:22:08.406822447Z",
     "start_time": "2025-06-20T01:59:50.172644Z"
    }
   },
   "source": [
    "# Add \"noise\" to speech\n",
    "data_original = [] # Original speech\n",
    "data_noisy = [] # Speech with added \"noise\" \n",
    "\n",
    "# STFT\n",
    "# Output dimensions should be divisible by factor 32\n",
    "n_fft = 1022\n",
    "hop = 63\n",
    "\n",
    "samples = 5000 # Number of samples\n",
    "\n",
    "for i in range(0, samples):\n",
    "    \n",
    "    # Get random speech\n",
    "    signal_index = np.random.randint(0, data_libri.shape[0])\n",
    "    signal_original = data_libri[signal_index]\n",
    "\n",
    "    # Signal power\n",
    "    signal_original_power = np.mean(signal_original ** 2)\n",
    "\n",
    "    # Calculate SNR\n",
    "    snr_db = 30\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    \n",
    "    # Calculate noise power\n",
    "    noise_power = signal_original_power / snr_linear\n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    noise = np.random.normal(0, np.sqrt(noise_power), np.shape(signal_original))\n",
    "\n",
    "    # Add noise to signal\n",
    "    signal_noisy = signal_original + noise\n",
    "\n",
    "    # STFT for original signal\n",
    "    st_ft = librosa.amplitude_to_db(np.abs(librosa.stft(signal_original, n_fft=n_fft, hop_length=hop)), ref=np.max)\n",
    "    data_original.append(st_ft)\n",
    "\n",
    "    # STFT for noisy signal\n",
    "    st_ft = librosa.amplitude_to_db(np.abs(librosa.stft(signal_noisy, n_fft=n_fft, hop_length=hop)), ref=np.max)\n",
    "    data_noisy.append(st_ft)\n",
    "\n",
    "# Samples, (STFT)\n",
    "print('data_origi size:', np.shape(data_original))\n",
    "print('data_noisy size:', np.shape(data_noisy))\n",
    "\n",
    "# Save to h5 file\n",
    "hf = h5py.File('dataset.h5', 'w')\n",
    "hf.create_dataset('data_original', data=np.float32(data_original))\n",
    "hf.create_dataset('data_noisy', data=np.float32(data_noisy))\n",
    "hf.close()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5000\u001B[39m \u001B[38;5;66;03m# Number of samples\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, samples):\n\u001B[1;32m     13\u001B[0m     \n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# Get random speech\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     signal_index \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, data_libri\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     16\u001B[0m     signal_original \u001B[38;5;241m=\u001B[39m data_libri[signal_index]\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Signal power\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from h5 file\n",
    "hf = h5py.File('dataset.h5', 'r')\n",
    "\n",
    "data_original = hf.get('data_original')\n",
    "data_original = np.array(data_original)\n",
    "\n",
    "data_noisy = hf.get('data_noisy')\n",
    "data_noisy = np.array(data_noisy)\n",
    "\n",
    "# Samples, (STFT)\n",
    "print('data_original size:', np.shape(data_original))\n",
    "\n",
    "# Samples, (STFT)\n",
    "print('data_noisy size:', np.shape(data_noisy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.pcolormesh(data_original[0], shading='auto')\n",
    "plt.title('Original')\n",
    "plt.xlabel('Window')\n",
    "plt.ylabel('FFT bin')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot noisy\n",
    "plt.subplot(1,3,2)\n",
    "plt.pcolormesh(data_noisy[0], shading='auto')\n",
    "plt.title('Noisy')\n",
    "plt.xlabel('Window')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "data_input_sc = scaler.fit_transform(data_noisy.reshape(-1, data_noisy.shape[-1])).reshape(data_noisy.shape)\n",
    "data_output_sc = scaler.fit_transform(data_original.reshape(-1, data_original.shape[-1])).reshape(data_original.shape)\n",
    "\n",
    "# Split into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_input_sc, data_output_sc, test_size=0.2)\n",
    "\n",
    "# Split into train and valid\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "# Sizes\n",
    "print('Dims')\n",
    "X_train = np.expand_dims(X_train, 1)\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "X_test = np.expand_dims(X_test, 1)\n",
    "y_test = np.expand_dims(y_test, 1)\n",
    "X_val = np.expand_dims(X_val, 1)\n",
    "y_val = np.expand_dims(y_val, 1)\n",
    "\n",
    "print('Train X:', np.shape(X_train))\n",
    "print('Train Y:', np.shape(y_train))\n",
    "print('Test X:', np.shape(X_test))\n",
    "print('Test Y:', np.shape(y_test))\n",
    "print('Val X:', np.shape(X_val))\n",
    "print('Val Y:', np.shape(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.batch1 = torch.nn.BatchNorm2d(32)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.norm1 = torch.nn.Dropout2d(0.15)\n",
    "    \n",
    "        # Flatten\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = torch.nn.Linear(#calculate#, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Check model summary\n",
    "model_enc = EncoderModel()\n",
    "summary(model_enc, input_size=(1, 1, 512, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = torch.nn.Linear(1, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, #calculate#)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.batch1 = torch.nn.BatchNorm2d(32)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.norm1 = torch.nn.Dropout2d(0.15)\n",
    "\n",
    "        # Head convolution\n",
    "        self.output = torch.nn.Conv2d(32, 1, kernel_size=1, stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear layers\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        # Reshape for Convolutional layers\n",
    "        x = x.reshape(x.shape[0], 128, 64, 16)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Head convolution\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Check model summary\n",
    "model_dec = DecoderModel()\n",
    "summary(model_dec, input_size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = EncoderModel()\n",
    "        self.decoder = DecoderModel()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Check model summary\n",
    "model = AutoEncoder()\n",
    "summary(model, input_size=(1, 1, 512, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for PyTorch model\n",
    "class data_generator(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, outputs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.outputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = AutoEncoder()\n",
    "model.to(device)\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "batch_size = 16\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = data_generator(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "valid_dataset = data_generator(X_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# For mixed precision training\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "# Functions\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train\n",
    "for e in range(num_epochs):\n",
    "\n",
    "    # Set to train mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        # Input\n",
    "        input = batch[0].to(device)\n",
    "        output = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Implement the mixed precision training\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            pred = model(input)\n",
    "            loss = loss_fn(pred, output)\n",
    "\n",
    "        # Izracunaj gradiente vseh tenzorjev\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Posodobi vrednosti utezi\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Train loss\n",
    "        train_loss += loss.item()*input.size(0)      \n",
    "        \n",
    "        # Remove unnecessary cache in CUDA memory\n",
    "        torch.cuda.empty_cache()\n",
    "        del pred, input, output\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # # Validate\n",
    "    # model.eval()\n",
    "    # valid_loss = 0.0\n",
    "       \n",
    "    # for i, batch in enumerate(valid_loader):\n",
    "\n",
    "    #    # Input\n",
    "    #    input = batch[0].to(device)\n",
    "    #    output = batch[1].to(device)\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         pred = model(input)\n",
    "    #         loss = loss_fn(pred, output)\n",
    "        \n",
    "    #     valid_loss += loss.item()*input.size(0) \n",
    "                \n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     del pred, input, output\n",
    "    \n",
    "    # valid_loss /= len(valid_loader)\n",
    "    \n",
    "    # Print\n",
    "    #print(f\"Epoch {e+1}/{num_epochs}\\tTrain loss: {train_loss:.4f}\\t Validation loss: {valid_loss:.4f}\\tlr: {curr_lr:.4f}\")\n",
    "    print(f\"Epoch {e+1}/{num_epochs}\\tTrain loss: {train_loss:.4f}\")\n",
    "\n",
    "torch.save(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we reconstruct signal from STFT?\n",
    "n_fft = 1022\n",
    "hop = 63\n",
    "data = data_libri[50]\n",
    "\n",
    "# Play original\n",
    "sd.play(np.array(data).flatten(), fs)\n",
    "sd.wait()\n",
    "\n",
    "# STFT\n",
    "# For reconstruction we need to save the phase as well\n",
    "st_ft = librosa.stft(data, n_fft=n_fft, hop_length=hop)\n",
    "st_ft_mag, st_ft_pha = librosa.magphase(st_ft)\n",
    "st_ft_mag = librosa.amplitude_to_db(st_ft_mag, ref=np.max)\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "st_ft_mag_scaled = scaler.fit_transform(st_ft_mag.reshape(-1, st_ft_mag.shape[-1])).reshape(st_ft_mag.shape)\n",
    "\n",
    "# Inverse scale\n",
    "st_ft_mag_scaled = scaler.inverse_transform(st_ft_mag_scaled.reshape(-1, st_ft_mag_scaled.shape[-1])).reshape(st_ft_mag_scaled.shape)\n",
    "\n",
    "# Inverse STFT\n",
    "st_ft_mag = librosa.db_to_amplitude(st_ft_mag_scaled, ref=17.0)\n",
    "st_ft = st_ft_mag * st_ft_pha\n",
    "reconstructed = librosa.core.istft(st_ft, hop_length=hop)\n",
    "reconstructed = np.array(reconstructed).flatten()\n",
    "\n",
    "# Play reconstructed\n",
    "sd.play(np.array(reconstructed).flatten(), fs)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test signal\n",
    "# Will be combined like: ...|speech|noisy speech|...|speech|noisy speech|...\n",
    "\n",
    "# STFT\n",
    "# Output dimensions should be divisible by factor 32\n",
    "n_fft = 1022\n",
    "hop = 63\n",
    "\n",
    "# Final signal\n",
    "test_signal = []\n",
    "test_signal_parts_mag = []\n",
    "test_signal_parts_pha = []\n",
    "\n",
    "# Append 10 signals\n",
    "signals = 10\n",
    "for i in range(0, signals):\n",
    "\n",
    "    # Get random speech\n",
    "    signal_index = np.random.randint(0, data_libri.shape[0])\n",
    "    signal_original = data_libri[signal_index]\n",
    "\n",
    "    # Signal power\n",
    "    signal_original_power = np.mean(signal_original ** 2)\n",
    "\n",
    "    # Calculate SNR\n",
    "    snr_db = 30\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    \n",
    "    # Calculate noise power\n",
    "    noise_power = signal_original_power / snr_linear\n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    noise = np.random.normal(0, np.sqrt(noise_power), np.shape(signal_original))\n",
    "\n",
    "    # Add noise to signal\n",
    "    signal_noisy = signal_original + noise\n",
    "\n",
    "    # Append\n",
    "    test_signal.append(signal_original)\n",
    "    test_signal.append(signal_noisy)\n",
    "\n",
    "    # Calculate STFT for original signal\n",
    "    # For reconstruction we need to save the phase as well\n",
    "    st_ft = librosa.stft(signal_original, n_fft=n_fft, hop_length=hop)\n",
    "    st_ft_mag, st_ft_pha = librosa.magphase(st_ft)\n",
    "    st_ft_mag = librosa.amplitude_to_db(st_ft_mag, ref=np.max)\n",
    "\n",
    "    test_signal_parts_mag.append(st_ft_mag)\n",
    "    test_signal_parts_pha.append(st_ft_pha)\n",
    "\n",
    "    # Calculate STFT for noisy signal\n",
    "    # For reconstruction we need to save the phase as well\n",
    "    st_ft = librosa.stft(signal_noisy, n_fft=n_fft, hop_length=hop)\n",
    "    st_ft_mag, st_ft_pha = librosa.magphase(st_ft)\n",
    "    st_ft_mag = librosa.amplitude_to_db(st_ft_mag, ref=np.max)\n",
    "\n",
    "    test_signal_parts_mag.append(st_ft_mag)\n",
    "    test_signal_parts_pha.append(st_ft_pha)\n",
    "\n",
    "test_signal = np.array(test_signal).flatten()\n",
    "test_signal_parts_mag = np.array(test_signal_parts_mag)\n",
    "test_signal_parts_pha = np.array(test_signal_parts_pha)\n",
    "\n",
    "# Samples\n",
    "print('test_signal size:', np.shape(test_signal))\n",
    "print('test_signal_parts_mag size:', np.shape(test_signal_parts_mag))\n",
    "print('test_signal_parts_pha size:', np.shape(test_signal_parts_pha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play what we have generated\n",
    "sd.play(test_signal, fs)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "test_signal_parts = scaler.fit_transform(test_signal_parts_mag.reshape(-1, test_signal_parts_mag.shape[-1])).reshape(test_signal_parts_mag.shape)\n",
    "\n",
    "# Add dimension\n",
    "test_signal_parts = np.expand_dims(test_signal_parts, 1)\n",
    "\n",
    "print(f\"test_signal_parts: {np.shape(test_signal_parts)}\")\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"model\", weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Clean signals from frames\n",
    "test_signal_parts_tensor = torch.from_numpy(np.float32(test_signal_parts)).to(device)\n",
    "predicted = model(test_signal_parts_tensor)\n",
    "\n",
    "# Remove second dimension\n",
    "predicted = predicted[:, 0, :, :]\n",
    "predicted = predicted.detach().cpu().numpy()\n",
    "\n",
    "# Inverse scale\n",
    "predicted = scaler.inverse_transform(predicted.reshape(-1, predicted.shape[-1])).reshape(predicted.shape)\n",
    "\n",
    "print('predicted:', np.shape(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1,4,1)\n",
    "plt.pcolormesh(test_signal_parts_mag[0], shading='auto')\n",
    "plt.title('Original')\n",
    "plt.xlabel('Window')\n",
    "plt.ylabel('FFT bin')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot noisy\n",
    "plt.subplot(1,4,2)\n",
    "plt.pcolormesh(test_signal_parts_mag[1], shading='auto')\n",
    "plt.title('Noisy')\n",
    "plt.xlabel('Window')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot predicted original\n",
    "plt.subplot(1,4,3)\n",
    "plt.pcolormesh(predicted[0], shading='auto')\n",
    "plt.title('Predicted/Original')\n",
    "plt.xlabel('Window')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot predicted noisy\n",
    "plt.subplot(1,4,4)\n",
    "plt.pcolormesh(predicted[1], shading='auto')\n",
    "plt.title('Predicted/Noisy')\n",
    "plt.xlabel('Window')\n",
    "plt.colorbar(label='Power [dB]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse STFT\n",
    "st_ft_mag = librosa.db_to_amplitude(predicted, ref=17.0)\n",
    "st_ft = st_ft_mag * test_signal_parts_pha\n",
    "reconstructed = librosa.core.istft(st_ft, hop_length=hop)\n",
    "reconstructed = np.array(reconstructed).flatten()\n",
    "\n",
    "print('reconstructed:', np.shape(reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(np.array(reconstructed).flatten(), fs)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(test_signal, fs)\n",
    "sd.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
